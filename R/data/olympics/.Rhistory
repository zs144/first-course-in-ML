beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)")
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,2.5),
xlab="r",ylab="p(r)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## coin_scenario1.R
# From A First Course in Machine Learning, Chapter 2.
# Francois-Xavier Briol, 30/03/16 [f-x.briol@warwick.ac.uk]
# Coin game, prior scenario 1
rm(list=ls(all=TRUE))
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = read.csv(file="big_data.csv",header=FALSE)
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 1
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)")
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,3),
xlab="r",ylab="p(r)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
rm(list=ls(all=TRUE))
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = read.csv(file="big_data.csv",header=FALSE)
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 1
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,3))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,3),
xlab="r",ylab="p(r)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,3),
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,3),
xlab="r",ylab="p(r|...)")
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,3),
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,3),
xlab="r",ylab="p(r|...)")
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,5),
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,5),
xlab="r",ylab="p(r|...)")
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
rm(list=ls(all=TRUE))
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = read.csv(file="big_data.csv",header=FALSE)
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 1
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),pch=2,
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),pch=2,
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 10 and 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2
xlab="r",ylab="p(r)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## coin_scenario1.R
# From A First Course in Machine Learning, Chapter 2.
# Francois-Xavier Briol, 30/03/16 [f-x.briol@warwick.ac.uk]
# Coin game, prior scenario 1
rm(list=ls(all=TRUE))
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = read.csv(file="big_data.csv",header=FALSE)
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 1
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 1020 tosses")
N = length(big_data)
post_alpha = post_alpha + sum(big_data)
post_beta = post_beta + N - sum(big_data)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
post_alpha
post_beta
sum(big_data)
N
N = length(big_data)
rm(list=ls(all=TRUE))
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = as.vector(unlist(read.csv(file="big_data.csv",header=FALSE)))
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 1
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
## Incorpoate another 1000
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 1020 tosses")
N = length(big_data)
post_alpha = post_alpha + sum(big_data)
post_beta = post_beta + N - sum(big_data)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
alpha = 50
beta = 50
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## Plot the prior density
alpha = 50
beta = 50
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
rm(list=ls(all=TRUE))
## Load the coin data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = as.vector(unlist(read.csv(file="big_data.csv",header=FALSE)))
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 1
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
rm(list=ls(all=TRUE))
## Load the coin data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = as.vector(unlist(read.csv(file="big_data.csv",header=FALSE)))
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 50
beta = 50
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
## coin_scenario1.R
# From A First Course in Machine Learning, Chapter 2.
# Francois-Xavier Briol, 30/03/16 [f-x.briol@warwick.ac.uk]
# Coin game, prior scenario 1
rm(list=ls(all=TRUE))
## Load the coin data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = as.vector(unlist(read.csv(file="big_data.csv",header=FALSE)))
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 1
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
## coin_scenario2.R
# From A First Course in Machine Learning, Chapter 2.
# Francois-Xavier Briol, 30/03/16 [f-x.briol@warwick.ac.uk]
# Coin game, prior scenario 2
rm(list=ls(all=TRUE))
## Load the coin data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = as.vector(unlist(read.csv(file="big_data.csv",header=FALSE)))
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 50
beta = 50
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
## Incorpoate another 1000
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 1020 tosses")
N = length(big_data)
post_alpha = post_alpha + sum(big_data)
post_beta = post_beta + N - sum(big_data)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
rm(list=ls(all=TRUE))
## Load the coin data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/coin_data")
big_data = as.vector(unlist(read.csv(file="big_data.csv",header=FALSE)))
toss_data = as.vector(unlist(read.csv(file="toss_data.csv",header=FALSE)))
toss_data2 = as.vector(unlist(read.csv(file="toss_data2.csv",header=FALSE)))
## Plot the prior density
alpha = 5
beta = 1
cat('\nPrior parameters: alpha:',alpha,'beta:',beta)
r = seq(0,1,0.01)
plot(r,dbeta(r,alpha,beta),type="l",xlab="r",ylab="p(r)",ylim=c(0,10))
## Incorporate the data one toss at a time
post_alpha = alpha
post_beta = beta
ch = c('T','H')
toss_string = c()
for(i in 1:length(toss_data)){
##
toss_string = c(toss_string,ch[toss_data[i]+1])
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main=paste("Posterior after",i,"tosses"))
post_alpha = post_alpha + toss_data[i]
post_beta = post_beta + 1 - toss_data[i]
points(r,dbeta(r,post_alpha,post_beta),type="l");
}
## Incorporate another ten
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 20 tosses")
N = length(toss_data2)
post_alpha = post_alpha + sum(toss_data2)
post_beta = post_beta + N - sum(toss_data2)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
## Incorpoate another 1000
plot(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),lty=2,
xlab="r",ylab="p(r|...)",main="Posterior after 1020 tosses")
N = length(big_data)
post_alpha = post_alpha + sum(big_data)
post_beta = post_beta + N - sum(big_data)
points(r,dbeta(r,post_alpha,post_beta),type="l",ylim=c(0,10),
xlab="r",ylab="p(r|...)")
# From A First Course in Machine Learning, Chapter 1.
# Francois-Xavier Briol, 22/03/16 [f-x.briol@warwick.ac.uk]
rm(list=ls(all=TRUE))
## Load the Olympic data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/olympics")
male100 = read.csv(file="data100m.csv",header=FALSE)
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/olympics")
male100 = read.csv(file="male100.csv",header=FALSE)
## Extract the male 100m data
x = male100[,1] # Olympic years
t = male100[,2] # Winning times
# Change the preceeding lines for different data.  e.g.
# x = male400[,1] # Olympic years
# t = male400[,2] # Winning times
# for the mens 400m event.
N = length(x) ## 27
## Compute the various averages required
# $\frac{1}{N}\sum_n x_n$
m_x = sum(x)/N
##
# $$\frac{1}{N}\sum_n t_n$$
#
m_t = sum(t)/N
##
# $\frac{1}{N}\sum_n t_n x_n$
m_xt = sum(t*x)/N
##
# $\frac{1}{N}\sum_n x_n^2$
m_xx = sum(x*x)/N
## Compute w1 (gradient) (Equation 1.10)
w_1 = (m_xt - m_x*m_t)/(m_xx - m_x^2)
## Compute w0 (intercept) (Equation 1.8)
w_0 = m_t - w_1*m_x
## Plot the data and linear fit
plot(x,t,xlab=c("Olympic year"),ylab=c("Winning time"))
xplot = c(min(x)-4,max(x)+4)
points(xplot,w_0+w_1*xplot,type="l")
