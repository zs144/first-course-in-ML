}
}
# Plot the means
for(k in 1:K){
points(cluster_means[k,1],cluster_means[k,2],pch=8,col=cols[k],cex=2)
}
}
## kmeansexample.R
# From A First Course in Machine Learning, Chapter 6.
# Francois-Xavier Briol, 01/06/16 [f-x.briol@warwick.ac.uk]
# Example of K-means clustering
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansdata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-3,5),ylim=c(-7,7))
## Randomly initialise the means
K = 3 # The number of clusters
cluster_means = matrix(runif(K*2),nrow=K)*10-5
## Iteratively update the means and assignments
converged = FALSE
N = nrow(X)
cluster_assignments = matrix(0,nrow=N,ncol=K)
di = matrix(0,nrow=N,ncol=K)
cols = c("red","darkgreen","blue")
while(!converged){
##
# Update assignments
for(k in 1:K){
di[,k] = rowSums((X - matrix(rep(cluster_means[k,],N),ncol=2,byrow=TRUE))^2)
}
old_assignments = cluster_assignments
cluster_assignments = (di == matrix(rep(apply(X=di,MARGIN=1,FUN=min),K),ncol=K,byrow=FALSE))
if(sum(sum(old_assignments!=cluster_assignments))==0){
converged = TRUE
}
# Plot the assigned data
plot(X[cluster_assignments[,1],1],X[cluster_assignments[,1],2],
col=cols[1],pch=16,main="Updated Assignments",xlim=c(-3,5),ylim=c(-7,7))
for(k in 1:K){
points(X[cluster_assignments[,k],1],X[cluster_assignments[,k],2],
col=cols[k],pch=16)
}
# Update means
for(k in 1:K){
if(sum(cluster_assignments[,k])==0){
# This cluster is empty, randomise it
cluster_means[k,] = runif(2)*10-5
} else {
current_assign = X[cluster_assignments[,k],]
current_assign = matrix(current_assign,ncol=ncol(X))
cluster_means[k,] = colMeans(current_assign)
}
}
# Plot the means
for(k in 1:K){
points(cluster_means[k,1],cluster_means[k,2],pch=8,col=cols[k],cex=2)
}
}
total_distance = matrix(0,nrow=length(Kvals),ncol=Nreps)
plot(X[,1],X[,2],pch=16,xlim=c(-3,5),ylim=c(-7,7))
Kvals = 1:10
Nreps = 50
total_distance = matrix(0,nrow=length(Kvals),ncol=Nreps)
di*cluster_assignments
sum(di*cluster_assignments)
# Overfitting in K-means
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansdata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-3,5),ylim=c(-7,7))
Kvals = 1:10
Nreps = 50
total_distance = matrix(0,nrow=length(Kvals),ncol=Nreps)
for(kv i 1:length(Kvals)){
for(rep in 1:Nreps){
cluster_means = matrix(runif(K*2),nrow=K)*10-5
converged = FALSE
N = nrow(X)
cluster_assignments = matrix(0,nrow=N,ncol=K)
di = matrix(0,nrow=N,ncol=K)
while(!converged){
# Update assignments
for(k in 1:K){
di[,k] = rowSums((X - matrix(rep(cluster_means[k,],N),ncol=2,byrow=TRUE))^2)
}
old_assignments = cluster_assignments
cluster_assignments = (di == matrix(rep(apply(X=di,MARGIN=1,FUN=min),K),ncol=K,byrow=FALSE))
if(sum(sum(old_assignments!=cluster_assignments))==0){
converged = TRUE
}
# Update means
for(k in 1:K){
if(sum(cluster_assignments[,k])==0){
# This cluster is empty, randomise it
cluster_means[k,] = runif(2)*10-5
} else {
current_assign = X[cluster_assignments[,k],]
current_assign = matrix(current_assign,ncol=ncol(X))
cluster_means[k,] = colMeans(current_assign)
}
}
}
# Compute the distance
total_distance[kv,rep] = sum(di*cluster_assignments)
}
}
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansdata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-3,5),ylim=c(-7,7))
Kvals = 1:10
Nreps = 50
total_distance = matrix(0,nrow=length(Kvals),ncol=Nreps)
for(kv i 1:length(Kvals)){
for(rep in 1:Nreps){
K = Kvals[kv]
cluster_means = matrix(runif(K*2),nrow=K)*10-5
converged = FALSE
N = nrow(X)
cluster_assignments = matrix(0,nrow=N,ncol=K)
di = matrix(0,nrow=N,ncol=K)
while(!converged){
# Update assignments
for(k in 1:K){
di[,k] = rowSums((X - matrix(rep(cluster_means[k,],N),ncol=2,byrow=TRUE))^2)
}
old_assignments = cluster_assignments
cluster_assignments = (di == matrix(rep(apply(X=di,MARGIN=1,FUN=min),K),ncol=K,byrow=FALSE))
if(sum(sum(old_assignments!=cluster_assignments))==0){
converged = TRUE
}
# Update means
for(k in 1:K){
if(sum(cluster_assignments[,k])==0){
# This cluster is empty, randomise it
cluster_means[k,] = runif(2)*10-5
} else {
current_assign = X[cluster_assignments[,k],]
current_assign = matrix(current_assign,ncol=ncol(X))
cluster_means[k,] = colMeans(current_assign)
}
}
}
# Compute the distance
total_distance[kv,rep] = sum(di*cluster_assignments)
}
}
# Overfitting in K-means
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansdata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-3,5),ylim=c(-7,7))
Kvals = 1:10
Nreps = 50
total_distance = matrix(0,nrow=length(Kvals),ncol=Nreps)
for(kv in 1:length(Kvals)){
for(rep in 1:Nreps){
K = Kvals[kv]
cluster_means = matrix(runif(K*2),nrow=K)*10-5
converged = FALSE
N = nrow(X)
cluster_assignments = matrix(0,nrow=N,ncol=K)
di = matrix(0,nrow=N,ncol=K)
while(!converged){
# Update assignments
for(k in 1:K){
di[,k] = rowSums((X - matrix(rep(cluster_means[k,],N),ncol=2,byrow=TRUE))^2)
}
old_assignments = cluster_assignments
cluster_assignments = (di == matrix(rep(apply(X=di,MARGIN=1,FUN=min),K),ncol=K,byrow=FALSE))
if(sum(sum(old_assignments!=cluster_assignments))==0){
converged = TRUE
}
# Update means
for(k in 1:K){
if(sum(cluster_assignments[,k])==0){
# This cluster is empty, randomise it
cluster_means[k,] = runif(2)*10-5
} else {
current_assign = X[cluster_assignments[,k],]
current_assign = matrix(current_assign,ncol=ncol(X))
cluster_means[k,] = colMeans(current_assign)
}
}
}
# Compute the distance
total_distance[kv,rep] = sum(di*cluster_assignments)
}
}
total_distance
boxplot(log(total_distance),xlab="K",ylab="Log D")
total_distance
total_distance
di
colSums(di*cluster_assignments)
sum(colSums(di*cluster_assignments))
sum(di*cluster_assignments)
## kmeansK.R
# From A First Course in Machine Learning, Chapter 6.
# Francois-Xavier Briol, 01/06/16 [f-x.briol@warwick.ac.uk]
# Overfitting in K-means
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansdata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-3,5),ylim=c(-7,7))
Kvals = 1:10
Nreps = 50
total_distance = matrix(0,nrow=length(Kvals),ncol=Nreps)
for(kv in 1:length(Kvals)){
for(rep in 1:Nreps){
K = Kvals[kv]
cluster_means = matrix(runif(K*2),nrow=K)*10-5
converged = FALSE
N = nrow(X)
cluster_assignments = matrix(0,nrow=N,ncol=K)
di = matrix(0,nrow=N,ncol=K)
while(!converged){
# Update assignments
for(k in 1:K){
di[,k] = rowSums((X - matrix(rep(cluster_means[k,],N),ncol=2,byrow=TRUE))^2)
}
old_assignments = cluster_assignments
cluster_assignments = (di == matrix(rep(apply(X=di,MARGIN=1,FUN=min),K),ncol=K,byrow=FALSE))
if(sum(sum(old_assignments!=cluster_assignments))==0){
converged = TRUE
}
# Update means
for(k in 1:K){
if(sum(cluster_assignments[,k])==0){
# This cluster is empty, randomise it
cluster_means[k,] = runif(2)*10-5
} else {
current_assign = X[cluster_assignments[,k],]
current_assign = matrix(current_assign,ncol=ncol(X))
cluster_means[k,] = colMeans(current_assign)
}
}
}
# Compute the distance
total_distance[kv,rep] = sum(di*cluster_assignments)
}
}
total_distance
boxplot(t(log(total_distance)),xlab="K",ylab="Log D")
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansnonlindata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-3,5),ylim=c(-7,7))
plot(X[,1],X[,2],xlim=c(-3,5),ylim=c(-7,7))
plot(X[,1],X[,2],xlim=c(-3,5),ylim=c(-7,7))
plot(X[,1],X[,2],xlim=c(-2,2),ylim=c(-2,2))
plot(X[,1],X[,2],pch=16,xlim=c(-2,2),ylim=c(-2,2))
Ke = rep(0,N)
N = nrow(X)
Ke = rep(0,N)
gam = 1
N = nrow(X)
Ke = rep(0,N)
gam = 1
for(n in 1:N){
for(n2 in 1:N){
Ke[n,n2] = exp(-gam*sum((X[n,]-X[n2,])^2))
}
}
X
N
N = nrow(X)
Ke = matrix(0,ncol=N,nrow=N)
gam = 1
for(n in 1:N){
for(n2 in 1:N){
Ke[n,n2] = exp(-gam*sum((X[n,]-X[n2,])^2))
}
}
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
s = colSums(X^2)
colSums(X^2)
s = rowSums(X^2)
pos = which(s==min(s))
Z[pos,] = c(0,1)
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
s = rowSums(X^2)
pos = which(s==min(s))
Z[pos,] = c(0,1)
K = 2 # The number of clusters
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
s = rowSums(X^2)
pos = which(s==min(s))
Z[pos,] = c(0,1)
di = matrix(0,nrow=N,ncol=K)
cols = c("red","blue")
Z[,1]
# Plot the assignments
plot(X[Z[,1],1],X[Z[,1],2],
col=cols[1],pch=16,main="Updated Assignments",xlim=c(-3,5),ylim=c(-7,7))
for(k in 1:K){
points(X[cluster_assignments[,k],1],X[cluster_assignments[,k],2],
col=cols[k],pch=16)
}
plot(X[Z[,1],1],X[Z[,1],2],
col=cols[1],pch=16,main="Updated Assignments",xlim=c(-3,5),ylim=c(-7,7))
for(k in 1:K){
points(X[Z[,k],1],X[Z[,k],2],
col=cols[k],pch=16)
}
## kernelkmeans.R
# From A First Course in Machine Learning, Chapter 6.
# Francois-Xavier Briol, 01/06/16 [f-x.briol@warwick.ac.uk]
# Kernel K-means
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansnonlindata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-2,2),ylim=c(-2,2))
## Compute the kernel
N = nrow(X)
Ke = matrix(0,ncol=N,nrow=N)
gam = 1
for(n in 1:N){
for(n2 in 1:N){
Ke[n,n2] = exp(-gam*sum((X[n,]-X[n2,])^2))
}
}
## Run Kernel K-means
converged = FALSE
# Assign all objects into one cluster except one
# Kernel K-means is *very* sensitive to initial conditions.  Try altering
# this initialisation to see the effect.
K = 2 # The number of clusters
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
s = rowSums(X^2)
pos = which(s==min(s))
Z[pos,] = c(0,1)
di = matrix(0,nrow=N,ncol=K)
cols = c("red","blue")
for(k in 1:K){
points(X[Z[,k],1],X[Z[,k],2],
col=cols[k],pch=16)
}
## kernelkmeans.R
# From A First Course in Machine Learning, Chapter 6.
# Francois-Xavier Briol, 01/06/16 [f-x.briol@warwick.ac.uk]
# Kernel K-means
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansnonlindata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-2,2),ylim=c(-2,2))
## Compute the kernel
N = nrow(X)
Ke = matrix(0,ncol=N,nrow=N)
gam = 1
for(n in 1:N){
for(n2 in 1:N){
Ke[n,n2] = exp(-gam*sum((X[n,]-X[n2,])^2))
}
}
## Run Kernel K-means
converged = FALSE
# Assign all objects into one cluster except one
# Kernel K-means is *very* sensitive to initial conditions.  Try altering
# this initialisation to see the effect.
K = 2 # The number of clusters
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
s = rowSums(X^2)
pos = which(s==min(s))
Z[pos,] = c(0,1)
di = matrix(0,nrow=N,ncol=K)
cols = c("red","blue")
for(k in 1:K){
points(X[Z[,k],1],X[Z[,k],2],
col=cols[k],pch=16)
}
dim(t(Z[,k]))
## Plot the assignments
for(k in 1:K){
pos = which(Z[,k])
points(X[pos,1],X[pos,2],col=cols[k],pch=16)
}
Z[,k]
## kernelkmeans.R
# From A First Course in Machine Learning, Chapter 6.
# Francois-Xavier Briol, 01/06/16 [f-x.briol@warwick.ac.uk]
# Kernel K-means
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansnonlindata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-2,2),ylim=c(-2,2))
## Compute the kernel
N = nrow(X)
Ke = matrix(0,ncol=N,nrow=N)
gam = 1
for(n in 1:N){
for(n2 in 1:N){
Ke[n,n2] = exp(-gam*sum((X[n,]-X[n2,])^2))
}
}
## Run Kernel K-means
converged = FALSE
# Assign all objects into one cluster except one
# Kernel K-means is *very* sensitive to initial conditions.  Try altering
# this initialisation to see the effect.
K = 2 # The number of clusters
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
s = rowSums(X^2)
pos = which(s==min(s))
Z[pos,] = c(0,1)
di = matrix(0,nrow=N,ncol=K)
cols = c("red","blue")
## Plot the assignments
for(k in 1:K){
pos = which(Z[,k]!=0)
points(X[pos,1],X[pos,2],col=cols[k],pch=16)
}
##
while(!converged){
Nk = colSums(Z)
for(k in 1:K){
# Compute kernelised distance
di[,k] = diag(Ke) - (2/(Nk[k]))*rowSums(matrix(rep(Z[,k],N),nrow=N,byrow=TRUE)*Ke)
di[,k] = di[,k] + (Nk[k]^(-2))*sum((Z[,k]%*%t(Z[,k]))*Ke)
}
oldZ = Z
Z = (di == matrix(rep(apply(X=di,MARGIN=1,FUN=min),K),ncol=K,byrow=FALSE))
if(sum(oldZ!=Z)==0){
converged = TRUE
}
## Plot the assignments
for(k in 1:K){
pos = which(Z[,k]!=0)
points(X[pos,1],X[pos,2],col=cols[k],pch=16)
}
}
## kernelkmeans.R
# From A First Course in Machine Learning, Chapter 6.
# Francois-Xavier Briol, 01/06/16 [f-x.briol@warwick.ac.uk]
# Kernel K-means
rm(list=ls(all=TRUE))
## Load the data
setwd("C:/Users/Francois-Xavier/Dropbox/OxWaSP/Thesis projects/A first course in machine learning/R_code/data/kmeansnonlindata/")
X = as.matrix(read.csv(file="X.csv",header=FALSE))
## Plot the data
plot(X[,1],X[,2],pch=16,xlim=c(-2,2),ylim=c(-2,2))
## Compute the kernel
N = nrow(X)
Ke = matrix(0,ncol=N,nrow=N)
gam = 1
for(n in 1:N){
for(n2 in 1:N){
Ke[n,n2] = exp(-gam*sum((X[n,]-X[n2,])^2))
}
}
## Run Kernel K-means
converged = FALSE
# Assign all objects into one cluster except one
# Kernel K-means is *very* sensitive to initial conditions.  Try altering
# this initialisation to see the effect.
K = 2 # The number of clusters
Z = matrix(rep(c(1,0),N),nrow=N,byrow=N)
s = rowSums(X^2)
pos = which(s==min(s))
Z[pos,] = c(0,1)
di = matrix(0,nrow=N,ncol=K)
cols = c("red","blue")
## Plot the assignments
for(k in 1:K){
pos = which(Z[,k]!=0)
points(X[pos,1],X[pos,2],col=cols[k],pch=16)
}
##
while(!converged){
Nk = colSums(Z)
for(k in 1:K){
# Compute kernelised distance
di[,k] = diag(Ke) - (2/(Nk[k]))*rowSums(matrix(rep(Z[,k],N),nrow=N,byrow=TRUE)*Ke)
di[,k] = di[,k] + (Nk[k]^(-2))*sum((Z[,k]%*%t(Z[,k]))*Ke)
}
oldZ = Z
Z = (di == matrix(rep(apply(X=di,MARGIN=1,FUN=min),K),ncol=K,byrow=FALSE))
if(sum(oldZ!=Z)==0){
converged = TRUE
}
## Plot the assignments
for(k in 1:K){
pos = which(Z[,k]!=0)
points(X[pos,1],X[pos,2],col=cols[k],pch=16)
}
}
mixture_means = matrix(c(3,3,1,-3),ncol=2,byrow=TRUE)
mixture_covs = list()
mixture_covs[[1]] = matrix(c(1,0,0,2),ncol=2,byrow=TRUE)
mixture_covs[[2]] = matrix(c(2,0,0,1),ncol=2,byrow=TRUE)
priors = c(0.7,0.3)
plotpoints = c(1:5,seq(10,30,5),40,50)
require(MASS) # might require to install this package: install.packages("MASS")
